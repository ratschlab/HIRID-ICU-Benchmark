import gin.torch.external_configurables
import icu_benchmarks.models.wrappers
import icu_benchmarks.models.encoders
import icu_benchmarks.models.utils
import icu_benchmarks.data.loader


EMB = 231
LR = 3e-4
HIDDEN = 128
HEADS = 1
LATENT = 2
NUM_CLASSES = 1
DEPTH = 1
DO = 0.1
DO_ATT = 0.1
BS = 16
EPOCHS = 1000
TASK = 'Dynamic_UrineOutput_2Hours_Reg'
RES = 1
RES_LAB = 1
MAXLEN = 2016
LOSS_WEIGHT = None


# Train params
train_common.model = @DLWrapper()
train_common.dataset_fn = @ICUVariableLengthDataset
train_common.data_path = PATH_TO_DL # TODO(User) update this
train_common.do_test = True


DLWrapper.encoder = @Transformer()
DLWrapper.loss = @mse_loss
DLWrapper.optimizer_fn = @Adam
DLWrapper.train.epochs = %EPOCHS
DLWrapper.train.batch_size = %BS
DLWrapper.train.patience = 10
DLWrapper.train.min_delta = 1e-4


ICUVariableLengthLoaderTables.splits = ['train','test','val']
ICUVariableLengthLoaderTables.task = %TASK
ICUVariableLengthLoaderTables.data_resampling = %RES
ICUVariableLengthLoaderTables.label_resampling = %RES_LAB
ICUVariableLengthDataset.maxlen = %MAXLEN
ICUVariableLengthDataset.scale_label = True

# Optimizer params
Adam.lr = %LR
Adam.weight_decay = 1e-6

# Encoder params
Transformer.emb = %EMB
Transformer.hidden = %HIDDEN
Transformer.heads = %HEADS
Transformer.ff_hidden_mult = %LATENT
Transformer.depth = %DEPTH
Transformer.num_classes = %NUM_CLASSES
Transformer.dropout = %DO
Transformer.dropout_att = %DO_ATT


